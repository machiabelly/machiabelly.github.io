import { Object3D, Vector4 } from 'three';
import { Constructor } from '../../../types/GlobalTypes';
interface FaceTrackingObjectAttributes {
    selfieMode: boolean;
    maxNumFaces: number;
    refineLandmarks: boolean;
    minDetectionConfidence: number;
    minTrackingConfidence: number;
}
export declare function CoreComputerVisionFaceParamConfig<TBase extends Constructor>(Base: TBase): {
    new (...args: any[]): {
        /** @param selfieMode */
        selfieMode: import("../../../engine/nodes/utils/params/ParamsConfig").ParamTemplate<import("../../../engine/index_all").ParamType.BOOLEAN>;
        /** @param Maximum number of faces to detect */
        maxNumFaces: import("../../../engine/nodes/utils/params/ParamsConfig").ParamTemplate<import("../../../engine/index_all").ParamType.INTEGER>;
        /** @param Whether to further refine the landmark coordinates around the eyes and lips, and output additional landmarks around the irises by applying the Attention Mesh Model */
        refineLandmarks: import("../../../engine/nodes/utils/params/ParamsConfig").ParamTemplate<import("../../../engine/index_all").ParamType.BOOLEAN>;
        /** @param Minimum confidence value ([0.0, 1.0]) from the face detection model for the detection to be considered successful. Default to 0.5. */
        minDetectionConfidence: import("../../../engine/nodes/utils/params/ParamsConfig").ParamTemplate<import("../../../engine/index_all").ParamType.FLOAT>;
        /** @param Minimum confidence value ([0.0, 1.0]) from the landmark-tracking model for the face landmarks to be considered tracked successfully, or otherwise face detection will be invoked automatically on the next input image. Setting it to a higher value can increase robustness of the solution, at the expense of a higher latency. Ignored if static_image_mode is true, where face detection simply runs on every image. Default to 0.5. */
        minTrackingConfidence: import("../../../engine/nodes/utils/params/ParamsConfig").ParamTemplate<import("../../../engine/index_all").ParamType.FLOAT>;
    };
} & TBase;
type AllowedSource = HTMLVideoElement | HTMLCanvasElement | HTMLImageElement;
interface ConvertedResult {
    multiFaceLandmarks: Vector4[];
}
type ConvertedResults = ConvertedResult[];
export declare class CoreComputerVisionFace {
    private static trackerByKey;
    private static trackerForObject;
    static trackMedia(object: Object3D, source: AllowedSource): void;
    static trackerResults(object: Object3D): ConvertedResults;
    private static _createTracker;
    static trackerKey(object: Object3D): string;
    static setAttributes(object: Object3D, options: FaceTrackingObjectAttributes): void;
}
export {};
